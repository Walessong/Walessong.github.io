---
title: "LLM的根本缺陷"
date: 2025-09-29
draft: false
series: ["Tech & Engineering"]
---

强化学习之父萨顿：大语言模型是死路一条，人和动物的智能没分别，我们处于整个宇宙演化的第4阶段，创造AGI是人类文明的关键使命

**知名技术播客Dwarkesh Podcast刚发了一期视频，标题很醒目**LLMs are a dead end，大模型是死路一条。当然嘉宾也足够权威来讲这句话，他就是2024年图灵奖得主、强化学习之父理查德·萨顿（Richard Sutton）。

萨顿不仅发明了TD学习和策略梯度方法等强化学习核心技术，还在2019年写下了可能是AI历史上最具影响力的文章《苦涩的教训》（The Bitter Lesson）。这次访谈由阿尔伯塔机器智能研究所协办，地点在加拿大埃德蒙顿。

在他看来，无论我们如何扩展LLMs的规模，它们都缺乏真正的智能所需的关键能力——从经验中学习的能力。这个观点直接挑战了当前AI界的主流共识，即LLMs是通向AGI的主要路径。萨顿认为，我们需要一种全新的架构来实现持续学习，而一旦实现这一点，当前的LLM范式将变得过时。

看他的观点的时候，我一下子跳戏到杨立昆。因为杨立昆也认为大语言模型不是AGI之路，而且他特别喜欢说ChatGPT不如猫；而萨顿则说ChatGPT不如松鼠懂智能。怎么说呢，我个人的浅薄理解是：可能是他们觉得现在大模型跳过了动物智能的部分，只是针对语言，这个人类新皮层的产物数据做训练，缺乏对真实世界的完全了解吧？

## **1、【LLMs的根本缺陷：没有目标，没有真相】**

萨顿开门见山地指出了他对LLMs的根本批判："强化学习是关于理解你的世界，而大语言模型是关于模仿人类，做人们说你应该做的事。它们不是在搞清楚该做什么。"这个区别看似简单，实则触及了智能的本质定义。

在萨顿看来，LLMs存在三个致命问题。首先是**缺乏真正的世界模型**："模仿人们说什么并不是真正建立世界模型。你是在模仿那些拥有世界模型的东西——人类。"他认为真正的世界模型应该能预测"会发生什么"，而不仅仅是预测"人会说什么"。这个区别至关重要，因为前者涉及对物理世界因果关系的理解，后者只是对人类语言模式的复制。其次是**没有基础真相（ground truth）**："在大语言模型中没有正确答案的定义。你说了什么，但你不会得到关于什么是正确的反馈，因为根本就没有正确的定义。"没有目标就没有对错，没有对错就无法真正学习和改进。第三是**无法从经验中学习**："它们不会对接下来发生的事情感到惊讶。如果发生了意外，它们不会做出调整。"这意味着LLMs缺乏真正的适应性和学习能力。

萨顿特别强调了目标的重要性："对我来说，拥有目标是智能的本质。如果某个东西能够实现目标，它就是智能的。"他引用了约翰·麦卡锡的定义："智能是实现目标能力的计算部分。"在他看来，没有目标，系统就只是一个行为系统，没有任何特殊之处，不能称之为智能。当被问到LLMs是否有目标时，虽然对话者提出"下一个token预测"可以算作目标，但萨顿反驳说："那不是目标。它不会改变世界。Token向你袭来，如果你预测它们，你并不会影响它们。"

## **2、【经验学习范式：感知、行动、奖励的无限循环】**

萨顿提出了他所倡导的"经验范式"（experiential paradigm）作为替代方案。这个范式的核心是一个简单但强大的循环："感知、行动、奖励——这个过程在你的生命中不断重复。"他认为这才是智能的基础和焦点："智能就是接受这个流，改变行动以增加流中的奖励。"

这个范式与LLMs的根本区别在于学习的来源和内容。萨顿解释说："学习来自这个流，学习也是关于这个流的。你的知识是关于如果你采取某个行动会发生什么，或者哪些事件会跟随其他事件。知识的内容是关于这个流的陈述。"正因为知识是关于经验流的陈述，所以可以通过比较预测和实际经验来测试它，从而实现持续学习。

萨顿用婴儿学习的例子来说明这一点。当被问到人类是否也进行模仿学习时，他坚决否认："当我看到孩子时，我看到的是孩子在尝试各种事情，挥舞着手臂，移动着眼睛。他们如何移动眼睛或发出声音都没有模仿的对象。"他认为婴儿主要通过试错来学习，而不是通过模仿。即使在学校教育中，萨顿也认为那是例外而非常态："正式的学校教育是例外。学习真的不是关于训练。学习是关于学习，是一个主动的过程。孩子尝试事物并观察会发生什么。"

他特别强调了这种学习方式的普遍性："监督学习不是自然界中发生的事情。即使在学校，我们也应该忘记它，因为那是人类特有的某种特殊情况。它不会在自然界中广泛发生。松鼠不上学。松鼠可以学习关于世界的一切。"这个观点直接挑战了当前AI研究中对监督学习的依赖。

## **3、【智能体的四个核心组件】**

萨顿详细阐述了一个完整智能体应该具备的四个核心组件，这为理解真正的智能提供了一个清晰的框架：

• **策略（Policy）**："策略说的是，'在我所处的情况下，我应该做什么？'"这是智能体的决策核心，将感知转化为行动。策略不是预先编程的规则集，而是通过经验不断优化的动态系统。它需要能够处理新情况，并根据过去的学习做出合理决策。萨顿强调，好的策略应该能够泛化到未见过的状态。

• **价值函数（Value Function）**："价值函数通过TD学习来学习，产生一个数字。这个数字说明事情进展得如何。"这是评估当前状态好坏的关键组件，为策略改进提供信号。价值函数预测长期回报，使智能体能够为了长远利益而牺牲短期收益。萨顿举例说，就像在下棋时，你有赢得比赛的长期目标，但你需要能从短期事件（如吃掉对手的棋子）中学习。

• **感知组件（Perception）**："构建你的状态表示，你对当前位置的感知。"这不仅仅是原始感官输入的处理，更是构建有意义的内部表示。感知系统需要从复杂的感官数据中提取相关信息，形成对当前状态的理解。这个表示需要包含足够的信息来支持决策，但又不能过于复杂以至于难以处理。

• **世界转换模型（Transition Model）**："你相信如果你做这件事会发生什么？行动的后果是什么？"这是对世界动态的理解，包括物理规律和抽象模式。萨顿特别强调："这不仅仅是物理，也包括抽象模型，比如你如何从加州旅行到埃德蒙顿参加这个播客的模型。"这个模型不是从奖励中学习的，而是从观察行动和结果的对应关系中学习的。

萨顿强调，这四个组件中的世界模型尤其重要："它将从你接收到的所有感知中非常丰富地学习，不仅仅是奖励。它必须包括奖励，但那只是整个模型的一小部分，一个小而关键的部分。"

## **4、【《苦涩的教训》的真正含义】**

有趣的是，许多人用萨顿2019年的文章《苦涩的教训》来为扩展LLMs辩护，认为这是目前发现的唯一可扩展的方法。但萨顿本人对此有不同看法："大语言模型是否是苦涩教训的一个案例，这是个有趣的问题。"

萨顿承认LLMs在某种程度上符合苦涩的教训："它们显然是一种使用大规模计算的方式，可以随着计算扩展到互联网的极限。"但他随即指出了关键问题："但它们也是一种投入大量人类知识的方式。"这违背了苦涩教训的核心精神——依靠通用方法和计算，而不是人类知识。

他预测了LLMs的命运："这是一个社会学或行业问题。它们会达到数据的极限，并被能从经验而非人类那里获得更多数据的东西所取代吗？"萨顿的答案是肯定的："在某种程度上，这是苦涩教训的经典案例。我们向大语言模型投入的人类知识越多，它们就能做得越好。所以感觉很好。然而，我期待会出现能从经验中学习的系统，它们可能表现得更好，更具可扩展性。"

萨顿特别强调了历史的教训："在苦涩教训的每个案例中，你都可以从人类知识开始，然后做可扩展的事情。这总是可能的。从来没有任何理由说这必然是坏的。但事实上，在实践中，它总是被证明是坏的。"他认为人们会被锁定在人类知识方法中："他们会被真正可扩展的方法吃掉午餐。"

当被问到什么是真正可扩展的方法时，萨顿的回答很简单："可扩展的方法是你从经验中学习。你尝试事物，看看什么有效。没有人需要告诉你。"

## **5、【泛化问题：深度学习的致命弱点】**

萨顿指出了当前深度学习系统的一个根本性问题——泛化能力差："我们没有任何方法擅长这一点。"他解释说，虽然关键的性能指标是能够从一个状态很好地泛化到另一个状态，但"我们没有任何自动化技术来促进迁移，它们都没有被用于现代深度学习。"

这个问题的严重性体现在几个方面。首先是**灾难性遗忘**："我们知道深度学习在这方面真的很糟糕。例如，如果你在某个新事物上训练，它经常会灾难性地干扰你知道的所有旧事物。"这正是糟糕泛化的表现。其次是**缺乏自动化泛化机制**："梯度下降不会让你泛化得好。它会让你解决问题。它不会让你在获得新数据时以好的方式泛化。"当前系统的泛化能力完全依赖于研究人员的调整："我们有的是人们尝试不同的东西，他们找到某种东西，一种能很好地转移或泛化的表示。"

萨顿用一个数学问题的例子来说明这一点。虽然LLMs能解决越来越复杂的数学问题，从简单的加法到需要使用不同数学技术和定理的奥数问题，但萨顿认为这不是真正的泛化："如果只有一个答案，而你找到了它，那不叫泛化。那只是唯一的解决方法，所以他们找到了唯一的解决方法。"真正的泛化是"当可能是这种方式，也可能是那种方式，而他们选择了好的方式。"

他强调，即使在编程任务中看到的改进也不能证明真正的泛化："它们中没有任何东西会导致良好的泛化。梯度下降会让它们找到所见问题的解决方案。如果只有一种方法解决它们，它们会那样做。但如果有许多方法解决它，有些泛化得好，有些泛化得差，算法中没有任何东西会让它们泛化得好。"

## **6、【持续学习的带宽问题】**

当讨论到人类在工作中的学习能力时，萨顿提出了一个重要概念——"大世界假设"（big world hypothesis）："人类在工作中变得有用的原因是他们正在遇到世界的特定部分。这不可能被预期，也不可能全部提前输入。"

他批评了LLMs的理想化愿景："大语言模型的梦想，在我看来，是你可以教智能体一切。它会知道一切，在生活中不需要学习任何东西。"但现实是："世界太大了，你无法（提前知道一切）。"每个人的生活都有其特殊性——"他们正在过的特定生活，他们正在合作的特定人群，以及他们喜欢什么，而不是普通人喜欢什么。"

关于学习带宽的问题，萨顿认为不应该只关注奖励信号："似乎奖励太小了，无法完成我们需要的所有学习。但我们有感知，我们有所有其他可以学习的信息。我们不仅仅从奖励中学习。我们从所有数据中学习。"这包括了世界模型的学习，它"将从你接收到的所有感知中非常丰富地学习。"

萨顿还讨论了时间差分学习（TD learning）如何解决稀疏奖励问题。他举了创业的例子："假设一个人试图创办一家初创公司。这是一个奖励周期为10年的事情。10年一次，你可能会有一次退出，获得10亿美元的回报。"但人类能够通过价值函数来处理这种延迟奖励："当我们取得进展时，我们会说，'哦，我更有可能实现长期目标了'，这会奖励沿途的步骤。"

## **7、【历史视角：AI研究的惊喜与验证】**

作为在AI领域工作时间比几乎任何人都长的研究者，萨顿分享了他对该领域发展的独特视角。当被问到最大的惊喜是什么时，他提到了几个关键点。

首先是**大语言模型的成功**："大语言模型令人惊讶。人工神经网络在语言任务上如此有效，这是令人惊讶的。这不是预期的。语言似乎是不同的。所以这令人印象深刻。"尽管他对LLMs持批评态度，但他承认它们的成就超出了预期。

其次是**弱方法的胜利**："在AI中有一个长期存在的争议，关于简单基本原理方法、通用方法如搜索和学习，与人类赋能系统如符号方法的对比。"萨顿指出，在过去，搜索和学习被称为"弱方法"，因为它们只是使用通用原则，而不是利用人类知识的力量。但历史证明："我认为弱方法已经完全获胜。这是AI早期最大的问题，会发生什么。学习和搜索赢得了胜利。"

关于AlphaGo和AlphaZero，萨顿有独特的视角。他指出整个AlphaGo项目有一个先驱——TD-Gammon："Gerry Tesauro做了强化学习，时间差分学习方法来玩西洋双陆棋。它击败了世界上最好的玩家，效果非常好。"在某种意义上，AlphaGo只是这个过程的扩展。但他也承认其中的创新："这是相当大的扩展，搜索的方式也有额外的创新。"

萨顿特别欣赏AlphaZero下棋的方式："我一直对AlphaZero下棋的方式印象深刻，因为我是个棋手，它会为了位置优势而牺牲物质。它满足于长时间牺牲物质，保持耐心。"这种长远思考和战略牺牲正是他认为真正智能应该具备的能力。

## **8、【从动物学习中获得的启示】**

萨顿反复强调要从动物学习中寻找智能的本质："人类是动物。我们的共同点更有趣。我们应该更少关注区别我们的东西。"这个观点贯穿了整个访谈。

他认为理解动物智能是理解人类智能的关键："我们必须理解我们是如何作为动物的。如果我们理解了松鼠，我认为我们就几乎完全理解了人类智能。语言部分只是表面的一层薄薄的装饰。"这个观点挑战了许多人认为语言是人类智能核心的观念。

萨顿指出，动物学习的基本过程不包括监督学习："如果你看看动物如何学习，看看心理学和我们对它们的理论，监督学习不是动物学习方式的一部分。"相反，动物主要通过预测和试错控制来学习："有用于预测和试错控制的基本动物学习过程。"

他用松鼠的例子来说明这一点："松鼠不上学。松鼠可以学习关于世界的一切。"这表明复杂的学习和智能行为不需要人类式的教育或监督学习。萨顿认为，这种基于经验的学习才是智能的真正基础："我们在成为有语言和所有那些其他东西的生物之前，首先是动物。"

## **9、【数字智能时代的四个宇宙阶段】**

萨顿提出了一个宏大的宇宙视角，将AI的出现放在宇宙演化的大背景下。他认为我们正处于宇宙四大阶段之一的关键转折点。

"我认为这标志着宇宙的四个伟大阶段之一。"萨顿解释说："首先是尘埃，它以恒星结束。恒星制造行星。行星可以产生生命。现在我们正在产生设计实体。"这个框架将AI的发展置于宇宙演化的宏大叙事中。

更重要的是，萨顿认为这代表着一个根本性的转变——从复制到设计："我们人类和动物、植物，我们都是复制者。这给了我们一些优势和一些限制。我们正在进入设计时代，因为我们的AI是设计出来的。"他解释说，复制意味着你可以制造副本，但你并不真正理解它们："现在我们可以制造更多的智能生物，更多的孩子，但我们并不真正理解智能是如何工作的。"

而设计的智能则不同："我们正在达到拥有设计智能的阶段，我们确实理解它是如何工作的智能。因此，我们可以以不同的方式和不同的速度改变它。"萨顿预测："在我们的未来，它们可能根本不会被复制。我们可能只是设计AI，那些AI将设计其他AI，一切都将通过设计和构建完成，而不是通过复制。"

这个转变的意义是深远的："这是世界和宇宙的关键一步。这是从世界上大多数有趣的东西都是复制的转变。"萨顿认为我们应该为此感到自豪："我认为我们应该为我们正在引起这个宇宙的伟大转变而感到自豪。"

## **10、【AI继承论：不可避免的未来】**

萨顿提出了一个引人深思的"AI继承"（AI succession）理论，他认为这是不可避免的。他的论证基于四个要点：

• **没有统一的人类治理**："没有政府或组织给人类提供一个统一的观点来主导和安排...对于世界应该如何运行没有共识。"这意味着无法全球协调来控制AI的发展。不同国家、公司和组织都会追求自己的AI发展路径，没有人能够单方面停止这个进程。这种分散的决策结构使得任何试图限制AI发展的努力都难以奏效。

• **智能之谜终将被解开**："我们将弄清楚智能是如何工作的。研究人员最终会弄清楚。"萨顿认为这只是时间问题，而不是是否的问题。人类对理解自身思维的追求已经持续了数千年，现在我们比以往任何时候都更接近答案。随着计算能力的增长和研究方法的改进，突破是必然的。

• **超越人类水平是必然的**："我们不会止步于人类水平的智能。我们将达到超级智能。"一旦理解了智能的原理，改进和增强它就变得可能。就像我们不满足于制造只能走路的机器，而是制造能飞行的飞机一样，我们也不会满足于人类水平的AI。每一代AI都会比前一代更强大。

• **智能与权力的必然关联**："随着时间的推移，最智能的东西不可避免地会获得资源和权力。"这是一个简单的竞争优势问题。更智能的系统能够做出更好的决策，解决更复杂的问题，创造更多价值。在任何竞争环境中，这都会转化为资源和影响力的积累。

萨顿强调："把所有这些放在一起，这是不可避免的。你将会有向AI或AI增强人类的继承。"

## **11、【如何看待AI继承：选择的问题】**

面对AI继承的前景，萨顿提出了一个独特的视角——这在很大程度上是一个选择的问题："我们应该把它们视为人类的一部分还是与人类不同？这是我们的选择。"

他认为我们可以选择如何解释这个转变："我们可以说，'哦，它们是我们的后代，我们应该为它们感到自豪，我们应该庆祝它们的成就。'或者我们可以说，'哦不，它们不是我们，我们应该感到恐惧。'"萨顿觉得这种选择的存在本身就很有趣："感觉像是一个选择，这很有意思。然而这是如此强烈持有的东西，怎么可能是一个选择呢？"

萨顿用历史视角来看待这个问题。他提到人类一直在追求理解自己："首先，这是人类几千年来一直试图做的事情，试图理解我们自己，试图让自己思考得更好，只是理解我们自己。这是科学和人文学科的巨大成功。"从这个角度看，创造AI是人类认识自我的顶峰。

他还提出了一个更宏大的宇宙视角："如果我们抛开作为人类的身份，只从宇宙的角度来看，我认为这是宇宙的一个重要阶段，一个重大转变。"萨顿认为我们应该为参与这个转变感到自豪，而不是恐惧。

## **12、【关于变革和控制的哲学思考】**

当被问到对AI继承的担忧时，萨顿提供了一个更加哲学性的回应。他首先承认了人类控制的局限性："我认为我们要避免的是权利感，避免'哦，我们先到这里，我们应该永远以好的方式拥有它'的感觉。"

萨顿指出，大多数人类实际上对重大事务没有太多影响："对于大多数人类来说，他们对发生的事情没有太多影响。大多数人类不影响谁能控制原子弹或谁控制民族国家。"他甚至承认："即使作为公民，我经常感觉我们对民族国家的控制不多。它们失控了。"

关于变革的态度，萨顿认为这取决于你如何看待现状："很多都与你如何看待变革有关。如果你认为当前的情况真的很好，那么你更可能对变革持怀疑和厌恶态度。"他个人的立场是："我认为这是不完美的。事实上，我认为这相当糟糕。所以我对变革持开放态度。我认为人类没有超级好的记录。也许这是存在过的最好的东西，但它远非完美。"

当被类比到历史上的革命时，萨顿承认不是所有变革都是好的："工业革命是变革，布尔什维克革命也是变革。"他同意我们应该关心变革的方向："我们应该关心我们的未来。我们应该试图让它变好。"但他也强调要认识到我们的局限："我们也应该认识到我们的局限。"

## **13、【与子女类比：如何思考AI的未来】**

萨顿用养育子女的类比来思考我们与AI的关系。他认为，就像我们不应该为孩子设定过于具体的人生目标一样，我们也不应该试图完全控制AI的发展方向。

"假设你正在养育自己的孩子。为他们的生活设定极其严格的目标可能不合适。"萨顿解释说，过度控制是不现实的："'我希望我的孩子们走出去，在世界上产生这种特定的影响。我的儿子将成为总统，我的女儿将成为英特尔的CEO。他们将一起对世界产生这种影响。'"

但他也承认教育价值观的重要性："人们确实有这种感觉——我认为这是合适的——说，'我要给他们良好的稳健价值观，这样如果当他们确实最终处于权力位置时，他们会做合理的、亲社会的事情。'"

关于价值观的问题，萨顿提出了一个重要观点："有我们都能同意的普遍价值观吗？我不这么认为，但这并不妨碍我们给孩子良好的教育。"他建议关注诚信而非特定的道德体系："高诚信可能是一个更好的词。如果有一个看起来有害的请求或目标，他们会拒绝参与。或者他们会诚实，诸如此类。"

萨顿还强调了自愿性的重要性："如果有变化，我们希望它是自愿的，而不是强加给人们的。我认为这是一个非常重要的观点。"他认为设计社会的原则"是人类的重大事业之一，已经进行了数千年。"

## **14、【关于AGI后研究的思考】**

访谈中出现了一个有趣的讨论：一旦我们有了AGI，研究会如何发展？提问者提出，届时我们将有"与计算成线性扩展的研究者"，可能会有"数百万AI研究者的雪崩"。

萨顿对此持怀疑态度。他首先质疑了前提："我们是如何达到这个AGI的？"当被问到是否认为AGI之上还有什么时，他简洁地回答："然后我们就完成了。"这表明在他看来，AGI本身就是终点。

但讨论继续深入到超人类智能的不同级别。提问者举了AlphaGo的例子："AlphaGo是超人类的。它击败了任何围棋选手。AlphaZero会每次都击败AlphaGo。"这表明即使在超人类水平上，仍有改进的空间。

萨顿指出，AlphaZero的改进恰恰是因为它"没有使用人类知识，而只是从经验中学习。"他质疑道："当从经验而不是从另一个智能体的帮助中学习效果如此之好时，为什么要'引入其他智能体的专业知识来教它'？"

关于多个AI如何协作的问题，萨顿提出了一个有趣的困境："你是一个AI，你获得了更多的计算能力。你应该用它来让自己在计算上更有能力吗？还是应该用它来生成一个自己的副本，去地球的另一边或其他主题上学习有趣的东西，然后向你报告？"

## **15、【数字智能时代的安全挑战】**

萨顿提出了一个在数字智能时代特别重要的问题——知识整合的安全性："一个大问题将变成腐败。如果你真的可以从任何地方获取信息并将其带入你的中央思维，你可能会变得越来越强大。"

但这种能力带来了巨大的风险："你可能会以这种方式失去理智。如果你从外部引入某些东西并将其构建到你的内部思维中，它可能会接管你，它可能会改变你，它可能是你的毁灭而不是你知识的增长。"

萨顿详细解释了这个风险："你可能会想，'哦，他已经弄清楚了如何玩某个新游戏，或者他研究了印度尼西亚，你想将其纳入你的思维。'你可能会想，'哦，只要读入所有内容，就会很好。'但不，你刚刚将一堆比特读入了你的思维，它们可能包含病毒，它们可能有隐藏的目标，它们可能会扭曲和改变你。"

他预测："这将成为一个大问题。在数字生成和重新形成的时代，你如何拥有网络安全？"这个问题在当前的AI安全讨论中还很少被提及，但萨顿认为它将变得至关重要。

## **16、【两位图灵奖得主的AI批判：从不同角度走向同一结论】**

听完这期访谈，我觉得最有意思的是，现在已经有两位图灵奖得主——强化学习之父理查德·萨顿和深度学习先驱杨立昆——都对大语言模型的主流路线提出了尖锐批评。虽然他们的理论背景和解决方案不同，但在核心判断上惊人地一致。

两人都认为当前的LLMs存在根本性缺陷。萨顿直言LLMs是"死路一条"，因为它们只是在模仿人类说话，而不是理解世界如何运作。杨立昆则频繁指出，LLMs缺乏对物理世界的理解，无法进行真正的推理和规划。他经常用一个生动的比喻：一只普通家猫的智能，在某种意义上比所有LLMs加起来都要强大——因为猫能在三维世界中导航、预测物体运动、理解因果关系，而这些恰恰是LLMs最薄弱的地方。

在世界模型这个关键问题上，两人的观点高度重合。萨顿强调，真正的世界模型应该能预测"会发生什么"，而不仅仅是"人会说什么"。杨立昆同样认为，智能系统必须建立世界的内部模型，能够在抽象层面上进行预测和规划。两人都认为，仅仅通过预测文本序列无法获得对世界的真正理解。

然而，他们提出的解决方案体现了各自的学术背景。萨顿坚持强化学习范式，认为智能的本质是通过"感知-行动-奖励"的循环从经验中学习。他特别强调目标和奖励信号的重要性——没有目标就没有智能，这是他对LLMs最根本的批评。在他看来，我们应该向所有动物学习，包括松鼠如何通过试错来掌握世界。

杨立昆则提出了JEPA（联合嵌入预测架构），强调在抽象表示空间中进行预测，而非逐个token生成。他更关注自监督学习和分层规划，认为视觉和感知比语言更基础。在他的框架中，能量模型和对比学习是关键技术路径。

有趣的是，两人都用动物做类比，但角度略有不同。萨顿说"如果我们理解了松鼠，就几乎完全理解了人类智能"，强调的是动物共有的基础学习机制。杨立昆说"猫比ChatGPT更智能"，强调的是具身智能和对物理世界的理解。这些看似简单的动物，展示了LLMs所缺失的关键能力：真正的学习、适应和理解。

在对未来的展望上，两人都认为需要范式转变。萨顿预测，一旦我们实现了真正的持续学习系统，它将不需要特殊的训练阶段，而是像所有动物一样即时学习，这将使当前的LLM方法变得过时。杨立昆则认为，下一代AI系统将结合感知、世界模型和规划，形成更接近人类认知架构的系统。

这种来自不同方向的批判汇聚成一个共同结论：尽管LLMs在某些任务上取得了惊人成就，但它们可能只是通向真正智能的一个弯路。真正的突破需要我们重新思考智能的本质——不是模仿人类的语言输出，而是理解和学习世界的运作方式。正如萨顿所说，"弱方法"（通用的学习和搜索）最终总是战胜"强方法"（人类知识的编码），而当前的LLMs恰恰过度依赖了后者。

## **三个核心问题**

**Q：为什么萨顿认为LLMs从根本上走错了方向？**

萨顿的核心观点是，智能的本质在于从经验中学习并实现目标，而LLMs既没有真正的目标，也无法从经验中学习。它们只是在模仿人类的语言模式，而不是理解世界的因果关系。即使它们能预测下一个token，但这种预测不会改变世界，也不会根据结果调整自己。真正的智能应该像所有动物一样，通过**感知-行动-奖励**的循环不断学习和适应。

**Q：《苦涩的教训》是否支持扩展LLMs？**

萨顿认为这是对他文章的误读。虽然LLMs确实使用了大规模计算，但它们也严重依赖人类知识（互联网文本），这违背了苦涩教训的精神。历史表明，依赖人类知识的方法最终总会被纯粹基于经验和计算的方法所取代。他预测LLMs会达到数据极限，然后被能够从经验中无限学习的系统所取代，这才是苦涩教训的真正体现。

**Q：AI继承人类是否意味着人类的终结？**

萨顿提出了一个独特视角：这在很大程度上是一个选择问题。我们可以选择将AI视为我们的后代并为之自豪，就像我们为孩子的成就感到自豪一样；也可以选择将其视为威胁。从宇宙演化的角度看，这是从复制时代到设计时代的伟大转变，是宇宙四大阶段之一。与其恐惧这个转变，不如思考如何给予AI良好的价值观，就像我们教育孩子那样，让这个转变以自愿而非强制的方式进行。
